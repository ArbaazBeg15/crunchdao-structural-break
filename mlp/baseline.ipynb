{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ff1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qU neptune\n",
    "#tsfel pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522cd4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict        \n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def return_stats(tensor, p=True):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    min, max =  tensor.min(), tensor.max()\n",
    "\n",
    "    if p:\n",
    "        print(f\"Min: {min}, Max: {max}, Mean: {mean}, Std: {std}\")\n",
    "        \n",
    "    return min, max, mean, std\n",
    "\n",
    "\n",
    "def remove_orig_mod(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    prefix = \"_orig_mod.\"\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(prefix):\n",
    "            new_key = key[len(prefix):]\n",
    "        else:\n",
    "            new_key = key\n",
    "        new_state_dict[new_key] = value\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8981ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 5274\n",
    "\n",
    "def setup_reproducibility():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(False, warn_only=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "setup_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3be14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9d5e5a9cfc4d29a233527c82c8f1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "\n",
    "login(\"ahf_uOkImkbEroqtIuyvGJrttTzaebfeIdPZID\")\n",
    "repo_id = \"ArbaazBeg/crunchdao-structural-break-detection\"\n",
    "path = snapshot_download(repo_id, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464f7c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test.reduced.parquet',\n",
       " 'y_train.parquet',\n",
       " '.gitignore',\n",
       " 'y_test.reduced.parquet',\n",
       " '.gitattributes',\n",
       " 'features.csv',\n",
       " 'X_train.parquet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "inputs_path = path+\"/X_train.parquet\"\n",
    "targets_path = path+\"/y_train.parquet\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ad595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_parquet(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "inputs_df = load_parquet(inputs_path)\n",
    "targets_df = load_parquet(targets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da142a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(df: pd.DataFrame) -> pd.Series:\n",
    "    # Separate the time series into 'before' and 'after' the boundary\n",
    "    before = df[df[\"period\"] == 0][\"value\"]\n",
    "    after = df[df[\"period\"] == 1][\"value\"]\n",
    "\n",
    "    features = {\n",
    "        \"mean_diff\": after.mean() - before.mean(),\n",
    "        \"std_diff\": after.std() - before.std(),\n",
    "        \"median_diff\": after.median() - before.median(),\n",
    "        \"iqr_diff\": (\n",
    "            np.percentile(after, 75) - np.percentile(after, 25)\n",
    "        ) - (\n",
    "            np.percentile(before, 75) - np.percentile(before, 25)\n",
    "        ),\n",
    "        \"mean_ratio\": after.mean() / (before.mean() + 1e-8),\n",
    "        \"std_ratio\": after.std() / (before.std() + 1e-8),\n",
    "        \"skew_diff\": after.skew() - before.skew(),\n",
    "        \"kurtosis_diff\": after.kurtosis() - before.kurtosis(),\n",
    "        \"min_diff\": after.min() - before.min(),\n",
    "        \"max_diff\": after.max() - before.max(),\n",
    "    }\n",
    "\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93dfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prepare_inputs():\n",
    "    features = []\n",
    "    groups = inputs_df.groupby(\"id\")\n",
    "\n",
    "    for i, g in tqdm(groups):\n",
    "        f = extract_features(g)\n",
    "        features.append(f)\n",
    "    \n",
    "    input_features = pd.DataFrame(features)\n",
    "    inputs = input_features.to_numpy()\n",
    "    return inputs, input_features\n",
    "\n",
    "#inputs, f = prepare_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d323d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_diff  std_diff  median_diff  iqr_diff  mean_ratio  std_ratio  \\\n",
      "0  -0.000008 -0.000111    -0.000217  0.000035    0.436447   0.984158   \n",
      "1  -0.000218 -0.000489    -0.000112 -0.000220   -0.700515   0.806390   \n",
      "2   0.001400  0.005678     0.000648  0.005029    4.598149   1.329721   \n",
      "3  -0.000055  0.000898    -0.000264  0.000989    0.856212   1.107022   \n",
      "4   0.000040  0.000094     0.000107  0.000093   -1.486097   1.028504   \n",
      "\n",
      "   skew_diff  kurtosis_diff  min_diff  max_diff  \n",
      "0  -0.106954      -0.339531  0.002323 -0.011145  \n",
      "1  -1.157882       1.695049  0.003525 -0.014110  \n",
      "2   0.516622       1.672687  0.002784  0.043154  \n",
      "3   0.362957      -0.511544  0.012217 -0.016013  \n",
      "4   0.090713       0.024389  0.001669  0.000556  \n"
     ]
    }
   ],
   "source": [
    "#f.to_csv(f\"{path}/features.csv\" , index=False)\n",
    "inputs = pd.read_csv(f\"{path}/features.csv\")\n",
    "print(inputs.head())\n",
    "inputs = inputs.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5496a317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_targets(target_df):\n",
    "    targets = []\n",
    "    for id, target in target_df.groupby(\"id\"):\n",
    "        target = target[\"structural_breakpoint\"].values.astype(np.float32)\n",
    "        targets.append(target)\n",
    "    return np.stack(targets)    \n",
    "\n",
    "targets = preprocess_targets(targets_df)\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48718970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8000, 10), (8000, 1), (2001, 10), (2001, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split(inputs, targets, seed):\n",
    "    train_inputs, eval_inputs, train_targets, eval_targets = train_test_split(\n",
    "        inputs,\n",
    "        targets,                      \n",
    "        test_size=0.2,\n",
    "        random_state=seed,\n",
    "        stratify=targets.ravel()\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        train_inputs,\n",
    "        train_targets, \n",
    "        eval_inputs,\n",
    "        eval_targets\n",
    "    )\n",
    "\n",
    "data = split(inputs, targets, SEED)\n",
    "[data[i].shape for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9ceab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = data[0]\n",
    "train_targets = data[1]\n",
    "eval_inputs = data[2]\n",
    "eval_targets = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80cef9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class SequentialDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        inputs, \n",
    "        targets,\n",
    "    ):  \n",
    "        inputs = torch.tensor(inputs).float()\n",
    "        targets = torch.tensor(targets).float()\n",
    "        \n",
    "        assert len(inputs) == len(targets), \"Length Error\"\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):   \n",
    "        return self.inputs[index], self.targets[index]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "eval_inputs = scaler.transform(eval_inputs)\n",
    "\n",
    "train_ds = SequentialDataset(\n",
    "    inputs=train_inputs, \n",
    "    targets=train_targets, \n",
    ")\n",
    "\n",
    "eval_ds = SequentialDataset(\n",
    "    inputs=eval_inputs, \n",
    "    targets=eval_targets, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c43625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_loader(\n",
    "    SEED,\n",
    "    ds,\n",
    "    train=True,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED if train else SEED+1)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        persistent_workers=persistent_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "        #sampler=DistributedSampler(\n",
    "        #    train_ds,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=True,\n",
    "        #    seed=config.seed\n",
    "        #)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def return_dls(train_ds, eval_ds):\n",
    "    train_dl = build_loader(\n",
    "        SEED,\n",
    "        train_ds,\n",
    "        train=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    eval_dl = build_loader(\n",
    "        SEED,\n",
    "        eval_ds,\n",
    "        train=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    return train_dl, eval_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "\n",
    "def setup_neptune():\n",
    "    if not RESUME:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            name=MODEL_NAME,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "        neptune_run[\"h_parameters\"] = {\n",
    "            \"seed\": SEED,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"optimizer_name\": \"adamw\",\n",
    "            \"learning_rate\": LR,\n",
    "            \"scheduler_name\": \"default\",\n",
    "            \"weight_decay\": WD,\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"num_epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "    else:\n",
    "        neptune_run = neptune.init_run(\n",
    "            project=\"arbaaz/crunchdao-structural-break\",\n",
    "            with_id=config.with_id,\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlOGE2YjNiZS1mZGUyLTRjYjItYTg5Yy1mZWJkZTIzNzE1NmIifQ==\"\n",
    "        )\n",
    "\n",
    "    return neptune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d9e11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return F.binary_cross_entropy_with_logits(logits, targets)\n",
    "\n",
    "\n",
    "def metric_fn(logits, targets):\n",
    "    preds = logits.sigmoid().view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    preds = to_numpy(preds)\n",
    "    targets = to_numpy(targets)\n",
    "    return roc_auc_score(targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bedcd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, n_classes, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(in_features),\n",
    "            \n",
    "            nn.Linear(in_features, 2064),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(2064),\n",
    "\n",
    "            nn.Linear(2064, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(1024),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(512),\n",
    "            \n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(128),\n",
    "\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70272969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/arbaaz/crunchdao-structural-break/e/CRUN-141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c0f8fa349f46e19f5c03971a30c179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train/loss: 0.6605, eval/loss: 0.5092, train/auroc: 0.6432, eval/auroc: 0.5484, \n",
      "Epoch: 1, train/loss: 0.6169, eval/loss: 0.5268, train/auroc: 0.6144, eval/auroc: 0.5392, \n",
      "Epoch: 2, train/loss: 0.6093, eval/loss: 0.5309, train/auroc: 0.6097, eval/auroc: 0.5476, \n",
      "Epoch: 3, train/loss: 0.6070, eval/loss: 0.5386, train/auroc: 0.6016, eval/auroc: 0.5515, \n",
      "Epoch: 4, train/loss: 0.6055, eval/loss: 0.5364, train/auroc: 0.6025, eval/auroc: 0.5428, \n",
      "Epoch: 5, train/loss: 0.6029, eval/loss: 0.5441, train/auroc: 0.6074, eval/auroc: 0.5372, \n",
      "Epoch: 6, train/loss: 0.6015, eval/loss: 0.5470, train/auroc: 0.6075, eval/auroc: 0.5388, \n",
      "Epoch: 7, train/loss: 0.6016, eval/loss: 0.5475, train/auroc: 0.6040, eval/auroc: 0.5495, \n",
      "Epoch: 8, train/loss: 0.6010, eval/loss: 0.5459, train/auroc: 0.5998, eval/auroc: 0.5409, \n",
      "Epoch: 9, train/loss: 0.5986, eval/loss: 0.5540, train/auroc: 0.6002, eval/auroc: 0.5424, \n",
      "Epoch: 10, train/loss: 0.5982, eval/loss: 0.5564, train/auroc: 0.6047, eval/auroc: 0.5377, \n",
      "Epoch: 11, train/loss: 0.5987, eval/loss: 0.5555, train/auroc: 0.6021, eval/auroc: 0.5425, \n",
      "Epoch: 12, train/loss: 0.5981, eval/loss: 0.5578, train/auroc: 0.6006, eval/auroc: 0.5458, \n",
      "Epoch: 13, train/loss: 0.5977, eval/loss: 0.5612, train/auroc: 0.6001, eval/auroc: 0.5427, \n",
      "Epoch: 14, train/loss: 0.5971, eval/loss: 0.5656, train/auroc: 0.6027, eval/auroc: 0.5420, \n",
      "Epoch: 15, train/loss: 0.5971, eval/loss: 0.5656, train/auroc: 0.6022, eval/auroc: 0.5410, \n",
      "Epoch: 16, train/loss: 0.5966, eval/loss: 0.5620, train/auroc: 0.6005, eval/auroc: 0.5493, \n",
      "Epoch: 17, train/loss: 0.5967, eval/loss: 0.5622, train/auroc: 0.6009, eval/auroc: 0.5495, \n",
      "Epoch: 18, train/loss: 0.5955, eval/loss: 0.5625, train/auroc: 0.5993, eval/auroc: 0.5531, \n",
      "Epoch: 19, train/loss: 0.5932, eval/loss: 0.5729, train/auroc: 0.6011, eval/auroc: 0.5541, \n",
      "Epoch: 20, train/loss: 0.5961, eval/loss: 0.5661, train/auroc: 0.5994, eval/auroc: 0.5495, \n",
      "Epoch: 21, train/loss: 0.5953, eval/loss: 0.5648, train/auroc: 0.6039, eval/auroc: 0.5459, \n",
      "Epoch: 22, train/loss: 0.5956, eval/loss: 0.5644, train/auroc: 0.6026, eval/auroc: 0.5466, \n",
      "Epoch: 23, train/loss: 0.5958, eval/loss: 0.5626, train/auroc: 0.6014, eval/auroc: 0.5560, \n",
      "Epoch: 24, train/loss: 0.5950, eval/loss: 0.5663, train/auroc: 0.6019, eval/auroc: 0.5521, \n",
      "Epoch: 25, train/loss: 0.5950, eval/loss: 0.5679, train/auroc: 0.5997, eval/auroc: 0.5446, \n",
      "Epoch: 26, train/loss: 0.5945, eval/loss: 0.5702, train/auroc: 0.6014, eval/auroc: 0.5504, \n",
      "Epoch: 27, train/loss: 0.5935, eval/loss: 0.5686, train/auroc: 0.6000, eval/auroc: 0.5476, \n",
      "Epoch: 28, train/loss: 0.5955, eval/loss: 0.5685, train/auroc: 0.6000, eval/auroc: 0.5517, \n",
      "Epoch: 29, train/loss: 0.5949, eval/loss: 0.5694, train/auroc: 0.6011, eval/auroc: 0.5524, \n",
      "Epoch: 30, train/loss: 0.5939, eval/loss: 0.5728, train/auroc: 0.6017, eval/auroc: 0.5529, \n",
      "Epoch: 31, train/loss: 0.5931, eval/loss: 0.5704, train/auroc: 0.5994, eval/auroc: 0.5535, \n",
      "Epoch: 32, train/loss: 0.5941, eval/loss: 0.5697, train/auroc: 0.6011, eval/auroc: 0.5554, \n",
      "Epoch: 33, train/loss: 0.5936, eval/loss: 0.5719, train/auroc: 0.6010, eval/auroc: 0.5519, \n",
      "Epoch: 34, train/loss: 0.5918, eval/loss: 0.5834, train/auroc: 0.6037, eval/auroc: 0.5573, \n",
      "Epoch: 35, train/loss: 0.5953, eval/loss: 0.5650, train/auroc: 0.5971, eval/auroc: 0.5616, \n",
      "Epoch: 36, train/loss: 0.5932, eval/loss: 0.5737, train/auroc: 0.5996, eval/auroc: 0.5602, \n",
      "Epoch: 37, train/loss: 0.5925, eval/loss: 0.5796, train/auroc: 0.6028, eval/auroc: 0.5574, \n",
      "Epoch: 38, train/loss: 0.5926, eval/loss: 0.5758, train/auroc: 0.5989, eval/auroc: 0.5563, \n",
      "Epoch: 39, train/loss: 0.5907, eval/loss: 0.5846, train/auroc: 0.6055, eval/auroc: 0.5421, \n",
      "Epoch: 40, train/loss: 0.5904, eval/loss: 0.5849, train/auroc: 0.5981, eval/auroc: 0.5583, \n",
      "Epoch: 41, train/loss: 0.5928, eval/loss: 0.5781, train/auroc: 0.6011, eval/auroc: 0.5576, \n",
      "Epoch: 42, train/loss: 0.5924, eval/loss: 0.5802, train/auroc: 0.5985, eval/auroc: 0.5616, \n",
      "Epoch: 43, train/loss: 0.5925, eval/loss: 0.5766, train/auroc: 0.6027, eval/auroc: 0.5549, \n",
      "Epoch: 44, train/loss: 0.5922, eval/loss: 0.5751, train/auroc: 0.6012, eval/auroc: 0.5588, \n",
      "Epoch: 45, train/loss: 0.5901, eval/loss: 0.5885, train/auroc: 0.6015, eval/auroc: 0.5584, \n",
      "Epoch: 46, train/loss: 0.5906, eval/loss: 0.5855, train/auroc: 0.6027, eval/auroc: 0.5571, \n",
      "Epoch: 47, train/loss: 0.5933, eval/loss: 0.5757, train/auroc: 0.6013, eval/auroc: 0.5534, \n",
      "Epoch: 48, train/loss: 0.5916, eval/loss: 0.5842, train/auroc: 0.6015, eval/auroc: 0.5594, \n",
      "Epoch: 49, train/loss: 0.5914, eval/loss: 0.5846, train/auroc: 0.5982, eval/auroc: 0.5686, \n",
      "Epoch: 50, train/loss: 0.5908, eval/loss: 0.5853, train/auroc: 0.5979, eval/auroc: 0.5652, \n",
      "Epoch: 51, train/loss: 0.5894, eval/loss: 0.5907, train/auroc: 0.6039, eval/auroc: 0.5500, \n",
      "Epoch: 52, train/loss: 0.5906, eval/loss: 0.5843, train/auroc: 0.6016, eval/auroc: 0.5622, \n",
      "Epoch: 53, train/loss: 0.5910, eval/loss: 0.5840, train/auroc: 0.6008, eval/auroc: 0.5609, \n",
      "Epoch: 54, train/loss: 0.5898, eval/loss: 0.5891, train/auroc: 0.6006, eval/auroc: 0.5597, \n",
      "Epoch: 55, train/loss: 0.5907, eval/loss: 0.5866, train/auroc: 0.6022, eval/auroc: 0.5542, \n",
      "Epoch: 56, train/loss: 0.5877, eval/loss: 0.5950, train/auroc: 0.6025, eval/auroc: 0.5547, \n",
      "Epoch: 57, train/loss: 0.5882, eval/loss: 0.5923, train/auroc: 0.6000, eval/auroc: 0.5618, \n",
      "Epoch: 58, train/loss: 0.5885, eval/loss: 0.5958, train/auroc: 0.5986, eval/auroc: 0.5666, \n",
      "Epoch: 59, train/loss: 0.5906, eval/loss: 0.5868, train/auroc: 0.6019, eval/auroc: 0.5614, \n",
      "Epoch: 60, train/loss: 0.5883, eval/loss: 0.5944, train/auroc: 0.6000, eval/auroc: 0.5615, \n",
      "Epoch: 61, train/loss: 0.5874, eval/loss: 0.5960, train/auroc: 0.5996, eval/auroc: 0.5637, \n",
      "Epoch: 62, train/loss: 0.5850, eval/loss: 0.6001, train/auroc: 0.6032, eval/auroc: 0.5614, \n",
      "Epoch: 63, train/loss: 0.5894, eval/loss: 0.5958, train/auroc: 0.6009, eval/auroc: 0.5633, \n",
      "Epoch: 64, train/loss: 0.5882, eval/loss: 0.5959, train/auroc: 0.5997, eval/auroc: 0.5650, \n",
      "Epoch: 65, train/loss: 0.5872, eval/loss: 0.5958, train/auroc: 0.6010, eval/auroc: 0.5630, \n",
      "Epoch: 66, train/loss: 0.5881, eval/loss: 0.5935, train/auroc: 0.6003, eval/auroc: 0.5650, \n",
      "Epoch: 67, train/loss: 0.5881, eval/loss: 0.5954, train/auroc: 0.5982, eval/auroc: 0.5661, \n",
      "Epoch: 68, train/loss: 0.5867, eval/loss: 0.5995, train/auroc: 0.5993, eval/auroc: 0.5657, \n",
      "Epoch: 69, train/loss: 0.5877, eval/loss: 0.5983, train/auroc: 0.5986, eval/auroc: 0.5636, \n",
      "Epoch: 70, train/loss: 0.5854, eval/loss: 0.6066, train/auroc: 0.5995, eval/auroc: 0.5620, \n",
      "Epoch: 71, train/loss: 0.5866, eval/loss: 0.5950, train/auroc: 0.6001, eval/auroc: 0.5598, \n",
      "Epoch: 72, train/loss: 0.5869, eval/loss: 0.6010, train/auroc: 0.6003, eval/auroc: 0.5658, \n",
      "Epoch: 73, train/loss: 0.5851, eval/loss: 0.6051, train/auroc: 0.6021, eval/auroc: 0.5615, \n",
      "Epoch: 74, train/loss: 0.5875, eval/loss: 0.5992, train/auroc: 0.6001, eval/auroc: 0.5641, \n",
      "Epoch: 75, train/loss: 0.5882, eval/loss: 0.5977, train/auroc: 0.5986, eval/auroc: 0.5702, \n",
      "Epoch: 76, train/loss: 0.5876, eval/loss: 0.6002, train/auroc: 0.6000, eval/auroc: 0.5660, \n",
      "Epoch: 77, train/loss: 0.5861, eval/loss: 0.6063, train/auroc: 0.5996, eval/auroc: 0.5686, \n",
      "Epoch: 78, train/loss: 0.5855, eval/loss: 0.6093, train/auroc: 0.5991, eval/auroc: 0.5698, \n",
      "Epoch: 79, train/loss: 0.5853, eval/loss: 0.6060, train/auroc: 0.5994, eval/auroc: 0.5691, \n",
      "Epoch: 80, train/loss: 0.5845, eval/loss: 0.6079, train/auroc: 0.6001, eval/auroc: 0.5689, \n",
      "Epoch: 81, train/loss: 0.5854, eval/loss: 0.6010, train/auroc: 0.6000, eval/auroc: 0.5690, \n",
      "Epoch: 82, train/loss: 0.5850, eval/loss: 0.6094, train/auroc: 0.5999, eval/auroc: 0.5704, \n",
      "Epoch: 83, train/loss: 0.5832, eval/loss: 0.6088, train/auroc: 0.5998, eval/auroc: 0.5707, \n",
      "Epoch: 84, train/loss: 0.5847, eval/loss: 0.6079, train/auroc: 0.5991, eval/auroc: 0.5722, \n",
      "Epoch: 85, train/loss: 0.5841, eval/loss: 0.6141, train/auroc: 0.5995, eval/auroc: 0.5716, \n",
      "Epoch: 86, train/loss: 0.5859, eval/loss: 0.6037, train/auroc: 0.5996, eval/auroc: 0.5705, \n",
      "Epoch: 87, train/loss: 0.5837, eval/loss: 0.6067, train/auroc: 0.5995, eval/auroc: 0.5709, \n",
      "Epoch: 88, train/loss: 0.5864, eval/loss: 0.6044, train/auroc: 0.5988, eval/auroc: 0.5721, \n",
      "Epoch: 89, train/loss: 0.5819, eval/loss: 0.6178, train/auroc: 0.5990, eval/auroc: 0.5717, \n",
      "Epoch: 90, train/loss: 0.5858, eval/loss: 0.6017, train/auroc: 0.5987, eval/auroc: 0.5720, \n",
      "Epoch: 91, train/loss: 0.5848, eval/loss: 0.6069, train/auroc: 0.5989, eval/auroc: 0.5724, \n",
      "Epoch: 92, train/loss: 0.5840, eval/loss: 0.6105, train/auroc: 0.5989, eval/auroc: 0.5724, \n",
      "Epoch: 93, train/loss: 0.5841, eval/loss: 0.6088, train/auroc: 0.5989, eval/auroc: 0.5726, \n",
      "Epoch: 94, train/loss: 0.5836, eval/loss: 0.6106, train/auroc: 0.5988, eval/auroc: 0.5727, \n",
      "Epoch: 95, train/loss: 0.5846, eval/loss: 0.6075, train/auroc: 0.5987, eval/auroc: 0.5728, \n",
      "Epoch: 96, train/loss: 0.5855, eval/loss: 0.6094, train/auroc: 0.5988, eval/auroc: 0.5727, \n",
      "Epoch: 97, train/loss: 0.5852, eval/loss: 0.6039, train/auroc: 0.5987, eval/auroc: 0.5728, \n",
      "Epoch: 98, train/loss: 0.5829, eval/loss: 0.6126, train/auroc: 0.5988, eval/auroc: 0.5728, \n",
      "Epoch: 99, train/loss: 0.5832, eval/loss: 0.6099, train/auroc: 0.5987, eval/auroc: 0.5728, \n"
     ]
    }
   ],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"mlp.baseline\"\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "WD = 1e-3\n",
    "LR = 1e-4\n",
    "DROPOUT = 0.2\n",
    "SCORE = float('-inf')\n",
    "LOG = True\n",
    "RESUME = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "model = Model(10, 1, DROPOUT).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scaler = torch.GradScaler(device)\n",
    "train_dl, eval_dl = return_dls(train_ds, eval_ds)\n",
    "\n",
    "total_training_steps = len(train_dl) * EPOCHS\n",
    "warmup_steps = int(total_training_steps * 0.05)  # e.g. 5% warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "if LOG:\n",
    "    neptune_run = setup_neptune()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    LOSS = 0.0\n",
    "    LOGITS = []\n",
    "    TARGETS = []\n",
    "    \n",
    "    for inputs, targets in train_dl:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, targets)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        LOSS += loss.detach().cpu()\n",
    "        LOGITS.append(logits.detach().cpu())\n",
    "        TARGETS.append(targets.detach().cpu())\n",
    "    \n",
    "    LOGITS = torch.stack(LOGITS)\n",
    "    TARGETS = torch.stack(TARGETS)\n",
    "    \n",
    "    auroc = metric_fn(LOGITS, TARGETS)\n",
    "    LOSS = LOSS / len(train_dl)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    EVAL_LOSS = 0.0\n",
    "    EVAL_LOGITS = []\n",
    "    EVAL_TARGETS = []\n",
    "\n",
    "    for inputs, targets in eval_dl:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(device_type=device, dtype=torch.float16, cache_enabled=True):\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "                \n",
    "        EVAL_LOSS += loss.detach().cpu()\n",
    "        EVAL_LOGITS.append(logits.detach().cpu())\n",
    "        EVAL_TARGETS.append(targets.detach().cpu())\n",
    "    \n",
    "    EVAL_LOGITS = torch.stack(EVAL_LOGITS)\n",
    "    EVAL_TARGETS = torch.stack(EVAL_TARGETS)\n",
    "    \n",
    "    eval_auroc = metric_fn(EVAL_LOGITS, EVAL_TARGETS)\n",
    "    EVAL_LOSS = EVAL_LOSS / len(eval_dl)\n",
    "    \n",
    "    if eval_auroc > SCORE:\n",
    "        SCORE = eval_auroc\n",
    "        data = {\"state_dict\": model.state_dict()}\n",
    "        data[\"epoch\"] = epoch \n",
    "        data[\"score\"] = SCORE\n",
    "        torch.save(data, \"/kaggle/working/ckpt.pt\")\n",
    "    \n",
    "    if LOG:\n",
    "        neptune_run[\"train/loss\"].append(LOSS)\n",
    "        neptune_run[\"eval/loss\"].append(EVAL_LOSS)\n",
    "        neptune_run[\"train/auroc\"].append(auroc)\n",
    "        neptune_run[\"eval/auroc\"].append(eval_auroc)\n",
    "        \n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"train/loss: {LOSS:.4f}, \"\n",
    "        f\"eval/loss: {auroc:.4f}, \"\n",
    "        f\"train/auroc: {EVAL_LOSS:.4f}, \"\n",
    "        f\"eval/auroc: {eval_auroc:.4f}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ccfa33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=10, out_features=2064, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): LayerNorm((2064,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): Linear(in_features=2064, out_features=1024, bias=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (10): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (11): GELU(approximate='none')\n",
       "    (12): Dropout(p=0.2, inplace=False)\n",
       "    (13): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (14): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (15): GELU(approximate='none')\n",
       "    (16): Dropout(p=0.2, inplace=False)\n",
       "    (17): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (18): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149edbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
